# -*- coding: utf-8 -*-
"""reecha_q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vDdUUKr-QvcsQGTtaXQJZDyGcEmbkyT_
"""

pip install num2words

import nltk
nltk.download('punkt')

###Code to extract tar.gz #####
import tarfile
my_tar = tarfile.open('20_newsgroups.tar.gz')
my_tar.extractall() 
my_tar.close()

### code to extract static quality score from file.txt####
import numpy as np
import pandas as pd
f = open("file.txt", "r")
content=f.read()
content = content.split()
score=[]
norm=0
for i in range(0,len(content)):
  if i%2 !=0:
    score.append(int(content[i]))
    norm=norm+int(content[i])

####code to create inverted list sorted by decreasing order of static quality score####
import os
import glob
sub_list=["alt.atheism","comp.graphics","comp.os.ms-windows.misc","comp.sys.ibm.pc.hardware","comp.sys.mac.hardware","comp.windows.x","misc.forsale","rec.autos","rec.motorcycles","rec.sport.baseball","rec.sport.hockey","sci.crypt","sci.electronics","sci.med","sci.space","soc.religion.christian","talk.politics.guns","talk.politics.mideast","talk.politics.misc","talk.religion.misc"]

static_score_dict={}
i=0
for item in sub_list:
  files  =(glob.glob("20_newsgroups/"+item+"/*"))
  names=[]
   
  for file1 in files:
    x=file1.rfind("/")
    names.append(file1[x+1:len(file1)])
    
  names=sorted(names)

  for name in (names):
    static_score_dict.update({name:score[i]})
    i=i+1

###Code to create inverted list ###
###posting list is sorted on the decreasing order of static score###
###each posting of a posting list of any term contains a list of [docid,static score of doc,tf]###

from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
porter = PorterStemmer()
Inverted_List={}
N=0
for item in ["talk.politics.misc","talk.religion.misc"]:
  files  =(glob.glob("20_newsgroups/"+item+"/*"))
  print(item)
  for file in files:
    N=N+1 
    x=file.rfind("/")

    length=len(file)
    file_name=file
    print(file_name)
    file=open(file, "r",encoding="utf8", errors='ignore')
    content=file.read()

    words=tokenizer.tokenize(content)
    stemmed=[]
    
    for word in words:
      stemmed.append(porter.stem(word))
    for stem in stemmed:
      if stem not in Inverted_List.keys():
        temp=[file_name[x+1:length],static_score_dict.get(file_name[x+1:length]),1]
        Inverted_List.update({stem:[temp]})

      else:
        list=Inverted_List.get(stem)
        flag=0
        k=0
        j=0
        for ITEM in list:
          if ITEM[0]==file_name[x+1:length]:
          
            flag=1
            j=k
            break
          k=k+1
        
        if flag==1:
          
          list[j][2]=list[j][2]+1
        
        else:
          element=[file_name[x+1:length],static_score_dict.get(file_name[x+1:length]),1]
          list.append(element)




import pickle

pickle_out = open("Inverted_List.pickle","wb")
pickle.dump(Inverted_List, pickle_out)
pickle_out.close()

import pickle
pickle_in = open("Inverted_List (1).pickle","rb")
Inverted_List = pickle.load(pickle_in)

INV={}
for key in Inverted_List.keys():
  list=Inverted_List.get(key)
  INV.update({key:sorted(list, key = lambda x: x[1],reverse=True)})

###code to calculate value of 'r'###
length=0
terms=0
max=0
min=999999999999
lengths=[]
for term in INV:
  length=length+ len(INV.get(term))
  terms=terms+1
  lengths.append(length)
  if len(INV.get(term)) >max:
    max=len(INV.get(term))
  if len(INV.get(term)) <min :
    min=len(INV.get(term))

print("total length ",length)
print("total terms ",terms)
print("Average lenght",length/terms)
norm_len=length/(max)
print("normalised length",norm_len)

#code to divide the terms of inverted list on the basis of df###
Q1=0.33*norm_len
Q2=0.66*norm_len
Rare=[]
Normal=[]
Frequent=[]
Highly_Frequent=[]
for term in INV.keys():
  length1=len(INV.get(term))
  if length1 >=Q2:
    Frequent.append(term)
  
  if length1 < Q2 and length1 >=Q1:
    Normal.append(term)
  if length1< Q1:
    Rare.append(term)
    
 
print(len(Rare))
print(len(Normal))
print(len(Frequent))

cases=[[3,0,0],[0,3,0],[0,0,3],[0,1,2],[0,2,1],[1,0,2],[1,2,0],[2,0,1],[2,1,0],[1,1,1]]
minpos=[0,0,0,0,0,0,0,0]
from random import sample

L_H=[]
  #min=999999
for r in range(18,26):
  
  low_hits=0
  for case in cases:
    str=' '
    query=[]
    d1=(sample(Rare,case[0]))
    for d in d1:
      query.append(d)
    d2=(sample(Frequent,case[1]))
    for d in d2:
      query.append(d)
        
    d3=(sample(Normal,case[2]))
    for d in d3:
      query.append(d)
      
      
    low_hits=low_hits+func1(query,r)    
  L_H.append(low_hits)
  if low_hits <min:
    min=low_hits
pos = L_H.index(min)
minpos[pos]=minpos[pos]+1 

print(L_H)

print(L_H)

###Functions###

def func1(str,r):
  Term_HL=[]

  for term in INV.keys():
    H=[]
    L=[]
    list=INV.get(term)
    sorted_list=sorted(list, key = lambda x: x[2],reverse=True)
    for i in range(0,r):   ### r=20 ###
      if len(sorted_list) >i:
        H.append(sorted_list[i][0])
      else:
        break
    for j in range(i,len(sorted_list)):
      L.append(sorted_list[j][0])
    
    
    Term_HL.append(tuple([term,H,L]))
  #print(Term_HL[0])  
  low_hits=func2(Term_HL,str)
  print("hi",low_hits)
  return low_hits


#### Processing query ####

### Foreign query words are handled by "try-except" ###

from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import numpy as np 
import operator
def func2(Term_HL,query):
  low_hits=0
  N=20000
  
  
  K=40

  Q_T=query
        
  print(Q_T)  
  docs1=[] 
  docs=[]
  score=[]
  for term in Q_T:
    for item in Term_HL:
      if item[0]==term:
        for d in item[1]:
          docs.append(d)
      
  docs=np.unique(docs)
  Q_T_unique=np.unique(Q_T)


  query_vector=[]
  for qt in Q_T_unique:
    c=0
    for term in Q_T:
      if term == qt:
        c=c+1
    if INV.get(qt) != None:
      df=len(INV.get(qt))
      idf=np.log(N/df)
    else :
      idf=0
    query_vector.append((1+np.log(c)*idf))


  for doc in docs:
    vector=[]
    for qt in Q_T_unique:
      list=INV.get(qt)
      flag=0
      for item in list:
        if item[0]==doc:
          tf=item[2]
          df=len(INV.get(qt))
          flag=1
        
      if flag==0:
        vector.append(0)
      else:
        vector.append((np.log(N/df))*(1+np.log(tf)))
    score.append(tuple([doc,net_score(query_vector,vector,doc)]))
  score.sort(key = operator.itemgetter(1),reverse=True)

  if len(docs)>=K:
      #score=(score[0:K])
    print(score[0:K])

  else:
    low_hits=1
    for term in Q_T:
      for item in Term_HL:
        if item[0]==term:
            #docs1.append((item[2]))
          for d in item[2]:
            docs1.append(d)
      
    docs1=np.unique(docs1)
    for doc in docs1:
      vector=[]
      for qt in Q_T_unique:
        list=INV.get(qt)
        flag=0
        for item in list:
          if item[0]==doc:
            tf=item[2]
            df=len(INV.get(qt))
            flag=1
          
        if flag==0:
          vector.append(0)
        else:
          vector.append((np.log(N/df))*(1+np.log(tf)))
      score.append(tuple([doc,net_score(query_vector,vector,doc)]))
    score.sort(key = operator.itemgetter(1),reverse=True)

      
      #score=(score[0:K])
    print("low me se elements aaye hai")
    print(score[0:K])
  return (low_hits)

### INV is our final Inverted list ### 
### Have extracted High & Low list for each term from INV ###


###Term_HL is a list of tuples with each tuple is <term,High_list,Low_list>###
Term_HL=[]

for term in INV.keys():
  H=[]
  L=[]
  list=INV.get(term)
  sorted_list=sorted(list, key = lambda x: x[2],reverse=True)
  for i in range(0,20):   ### r=20 ###
    if len(sorted_list) >i:
      H.append(sorted_list[i][0])
    else:
      break
  for j in range(i,len(sorted_list)):
    L.append(sorted_list[j][0])
  
  Term_HL.append(tuple([term,H,L]))

print((Term_HL[0]))

#### Processing query ####

### Foreign query words are handled by "try-except" ###

from nltk.stem import PorterStemmer
from nltk.tokenize import RegexpTokenizer
import numpy as np 
import operator
N=20000
tokenizer = RegexpTokenizer(r'\w+')
porter = PorterStemmer()

try:
  query=input("enter you query: ")
  K=int(input("enter number of results you want: "))

  query_terms=tokenizer.tokenize(query)
  Q_T=[]
      
  for term in query_terms:
    Q_T.append(porter.stem(term))

  print(Q_T)  
  docs1=[] 
  docs=[]
  score=[]
  for term in Q_T:
    for item in Term_HL:
      if item[0]==term:
        for d in item[1]:
            docs.append(d)
    
  docs=np.unique(docs)
  Q_T_unique=np.unique(Q_T)


  query_vector=[]
  for qt in Q_T_unique:
    c=0
    for term in Q_T:
      if term == qt:
        c=c+1
    if INV.get(qt) != None:
      df=len(INV.get(qt))
      idf=np.log(N/df)
    else :
      idf=0
    query_vector.append((1+np.log(c)*idf))


  for doc in docs:
    vector=[]
    for qt in Q_T_unique:
      list=INV.get(qt)
      flag=0
      for item in list:
        if item[0]==doc:
          tf=item[2]
          df=len(INV.get(qt))
          flag=1
      
      if flag==0:
        vector.append(0)
      else:
        vector.append((np.log(N/df))*(1+np.log(tf)))
    score.append(tuple([doc,net_score(query_vector,vector,doc)]))
  score.sort(key = operator.itemgetter(1),reverse=True)

  if len(docs)>=K:
    #score=(score[0:K])
    print(score[0:K])

  else:
  
    for term in Q_T:
      for item in Term_HL:
        if item[0]==term:
          #docs1.append((item[2]))
          for d in item[2]:
            docs1.append(d)
    
    docs1=np.unique(docs1)
    for doc in docs1:
      vector=[]
      for qt in Q_T_unique:
        list=INV.get(qt)
        flag=0
        for item in list:
          if item[0]==doc:
            tf=item[2]
            df=len(INV.get(qt))
            flag=1
        
        if flag==0:
          vector.append(0)
        else:
          vector.append((np.log(N/df))*(1+np.log(tf)))
      score.append(tuple([doc,net_score(query_vector,vector,doc)]))
    score.sort(key = operator.itemgetter(1),reverse=True)

    
    #score=(score[0:K])
    print("low me se elements aaye hai")
    print(score[0:K])

except:
  print("some error occured while processsing query")

norm=0
for item in static_score_dict.keys():
  norm=norm+static_score_dict.get(item)

def net_score(query_vector,vector,doc):
  query_vector=np.array(query_vector)
  vector=np.array(vector)
  static_score=static_score_dict.get(doc)/norm
  #static_score=0
  dot=np.dot((query_vector),(vector))
  mag1= np.sqrt(vector.dot(vector))
  mag2= np.sqrt(query_vector.dot(query_vector))
  return (static_score+(dot/(mag1*mag2)))