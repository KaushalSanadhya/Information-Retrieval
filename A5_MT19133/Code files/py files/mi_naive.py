# -*- coding: utf-8 -*-
"""IR5_MI_Naive.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vy2KVf5BOawhdVD2PmgEH1nK3FFMU9A4
"""

## Cell 0 ##

cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
import pickle
#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# UNCOMMENT these three pickle files for executing the code for (80:20) split. Give path to the pickled files.  #

#pickle_in = open("PATH OF classes_list(80:20).pickle FILE","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF Global(80:20).pickle FILE","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("PATH OF xTest(80:20).pickle FILE","rb")
#xTest =pickle.load(pickle_in)

#pickle_in = open("PATH OF MI(80:20).pickle FILE","rb")
#MI =pickle.load(pickle_in)
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# UNCOMMENT these three pickle files for executing the code for 70:30 split. Give path to the pickled files. #

#pickle_in = open("PATH OF classes_list(70:30).pickle FILE","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF Global(70:30).pickle FILE","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("PATH OF xTest(70:30).pickle FILE","rb")
#xTest =pickle.load(pickle_in)

#pickle_in = open("PATH OF MI(70:30).pickle FILE","rb")
#MI =pickle.load(pickle_in)

#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++





#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# UNCOMMENT these three pickle files for executing the code for (50:50) split. Give path to the pickled files. #

#pickle_in = open("PATH OF classes_list(50:50).pickle FILE","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF Global(50:50).pickle FILE","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("PATH OF xTest(50:50).pickle FILE","rb")
#xTest =pickle.load(pickle_in)

#pickle_in = open("PATH OF MI(50:50).pickle FILE","rb")
#MI =pickle.load(pickle_in)

#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#Execution sequence of this python  code using pickled results

# Cell 1 --> Cell 2 --> Cell 3 -->Cell 4-->cell 10->cell 11-->cell 12-->cell 13 -->cell 14
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

### Cell 1 ###
##importing  nltk##

import nltk
nltk.download('punkt')

#Cell 2 #
##installing num2words ##
pip install num2words

#Cell 3 #
#Importing all esentials#

from nltk.stem import PorterStemmer 
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize 
from sklearn.utils import shuffle
import numpy as np
import glob
import os
from num2words import num2words
from nltk.corpus import stopwords 
import pickle
import math
import operator
from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt 
import random 
from sklearn.model_selection import train_test_split
import pandas as pd  

ps = PorterStemmer() 
tokenizer = RegexpTokenizer(r'\w+')

### CELL 4  ##
## Download stopwords ##
import nltk
nltk.download('stopwords')

## Cell 5 ##
##Splitting into Test & Train RANDOMLY using Test Train Split and shuffle ###



Data=pd.DataFrame(columns = ["file_name", "class"])
cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]


for group in cat_list:
  for filename in glob.glob("drive/My Drive/IR4/"+group+"/*"):
    
    length=len(filename)
    j=0
    for k in range(length-1,-1,-1):
      if filename[k]=='/':
        j=k
        break
  
    Data=Data.append({"file_name":filename[j+1:len(filename)],"class":group},ignore_index=True)

##Shuffling and split ##
Data = shuffle(Data)

x=Data["file_name"]
y=Data["class"]
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.2, random_state = 12)

## Cell 6 ##
## Pre-Processing Train Docs ##

stop_words = set(stopwords.words('english')) 
cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
GLOBAL=[]
classes_list={}
for group in cat_list:
  global_doc=[]
  for filename in glob.glob("drive/My Drive/IR4/"+group+"/*"):
    
    length=len(filename)
    j=0
    for k in range(length-1,-1,-1):
      if filename[k]=='/':
        j=k
        break
    print(filename[j+1:len(filename)])
    if filename[j+1:len(filename)] in list(xTrain):
      if not os.path.isdir(filename):
        text = open(filename,"r+",errors='ignore') 
        var=(text.read())
        var=var.replace(',','')
        words_tokens=tokenizer.tokenize(var)
        words = [w for w in words_tokens if not w in stop_words] 
      
        Words=[]
        for w in words:
          if w.isdigit():
            Words.append(ps.stem(num2words(w)))
            GLOBAL.append(ps.stem(num2words(w)))
          else:
            Words.append(ps.stem(w))
            GLOBAL.append(ps.stem(w))
      global_doc.append(Words)
  classes_list.update({group:global_doc})


#pickle_out = open("drive/My Drive/classes_list(80:20).pickle","wb")
#pickle.dump(classes_list, pickle_out)
#pickle_out.close()


#pickle_out = open("drive/My Drive/Global(80:20).pickle","wb")
#pickle.dump(GLOBAL, pickle_out)
#pickle_out.close()

## Cell 7 ##
### Calculating MI for tuple(Term,Class)####

MI=[]

Global_unique=np.unique(GLOBAL) #all terms of all the classes

for term in Global_unique:
  for clas in cat_list:
    N11=0
    N10=0
    N01=0
    N00=0

    for key in classes_list.keys():
      docs=classes_list.get(key)
      for doc in docs:
        if term in doc and key ==clas:
          N11=N11+1
        if term in doc and key !=clas:
          N10=N10+1
        if term not in doc and key ==clas:
          N01=N01+1
        if term not in doc and key !=clas:
          N00=N00+1
    N00=N00+0.0001
    N01=N01+0.0001
    N10=N10+0.0001
    N11=N11+0.0001      
    N=N00+N11+N01+N10
    

    val=(N11/N)*math.log(((N*N11)/((N11+N10)*(N01+N11)))+0.0001,2) + (N01/N)*math.log(((N*N01)/((N01+N00)*(N01+N11)))+0.0001,2) + (N10/N)*math.log(((N*N10)/((N11+N10)*(N00+N10)))+0.0001,2) + (N00/N)*math.log(((N*N00)/((N01+N00)*(N00+N10)))+0.0001,2)
    print(term,val)
    MI.append(tuple([term,clas,val]))

#### selecting the value of number of features (K) ###
### Feature Selection using TF-IDF ###

## Cell 8##
#cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
#pickle_in = open("drive/My Drive/classes_list(70:30).pickle","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("drive/My Drive/Global(70:30).pickle","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("drive/My Drive/xTest(70:30).pickle","rb")
#xTest =pickle.load(pickle_in)

## Cell 9##
#MI=[]
#file = open("drive/My Drive/MI_1(70:30).pickle", 'rb')      
#MI1 = pickle.load(file) 
#file = open("drive/My Drive/MI_2(70:30).pickle", 'rb')      
#MI2 = pickle.load(file) 
#file = open("drive/My Drive/MI_3(70:30).pickle", 'rb')      
#MI3 = pickle.load(file) 
#file = open("drive/My Drive/MI_4(70:30).pickle", 'rb')      
#MI4 = pickle.load(file) 
##file = open("drive/My Drive/MI_5(70:30).pickle", 'rb')      
#MI5 = pickle.load(file) 
##file = open("drive/My Drive/MI_6(80:20).pickle", 'rb')      
#MI6 = pickle.load(file)

#for item in range(0,len(MI1)):
#  print(item)
#  MI.append(MI1[item])
#  MI.append(MI2[item])
#  MI.append(MI3[item])
#  MI.append(MI4[item])
#  MI.append(MI5[item])
 # MI.append(MI6[item])

#print(len(MI))
#print(len(np.unique(GLOBAL)))

## Cell 10##
## Further processing of MI.Please note that I hv selected 1/2 of total features i.e. 50% features on the basis of MI score of pair(class ,term)

MI1=[]
MI2=[]
MI3=[]
MI4=[]
MI5=[]

for item in MI:
  if item[1]==cat_list[0]:
    MI1.append(item)
  if item[1]==cat_list[1]:
    MI2.append(item)
  if item[1]==cat_list[2]:
    MI3.append(item)
  if item[1]==cat_list[3]:
    MI4.append(item)
  if item[1]==cat_list[4]:
    MI5.append(item)

MI1.sort(key=lambda elem : elem[2],reverse=True)
MI2.sort(key=lambda elem : elem[2],reverse=True)
MI3.sort(key=lambda elem : elem[2],reverse=True)
MI4.sort(key=lambda elem : elem[2],reverse=True)
MI5.sort(key=lambda elem : elem[2],reverse=True)

length=int(len(MI1)/2)

Mutual_info=[]
Mutual_info.append(MI1[0:length])
Mutual_info.append(MI2[0:length])
Mutual_info.append(MI3[0:length])
Mutual_info.append(MI4[0:length])
Mutual_info.append(MI5[0:length])

## Cell 11 ##
##processing Probability for implementing Naive Logic ##

denominator=[] 
probab=[]
keys=list(classes_list.keys())
for i in range(0,len(Mutual_info)):
  List=Mutual_info[i]
  d=0
  docs=classes_list.get(keys[i])
  for term in List:
    ct=0
    for doc in docs:
      ct=ct+doc.count(term[0])
    probab.append(tuple([term[0],keys[i],ct]))
    d=d+ct

  denominator.append(d)



probab1={}
probab2={}
probab3={}
probab4={}
probab5={}

for item in probab:
  if item[1]==cat_list[0]:
    probab1.update({item[0]:item[2]})
  if item[1]==cat_list[1]:
    probab2.update({item[0]:item[2]})
  if item[1]==cat_list[2]:
    probab3.update({item[0]:item[2]})
  if item[1]==cat_list[3]:
    probab4.update({item[0]:item[2]})
  if item[1]==cat_list[4]:
    probab5.update({item[0]:item[2]})

## Cell 12 ##
## Pre-processing Test Docs ##
stop_words = set(stopwords.words('english')) 
cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
m=0
docs_list=[]
for group in cat_list:
  for filename in glob.glob("drive/My Drive/IR4/"+group+"/*"):
    
    length=len(filename)
    j=0
    for k in range(length-1,-1,-1):
      if filename[k]=='/':
        j=k
        break
    #print(filename[j+1:len(filename)])
    if filename[j+1:len(filename)] in list(xTest):
      m=m+1
      print(m)
      if not os.path.isdir(filename):
        text = open(filename,"r+",errors='ignore') 
        var=(text.read())
        var=var.replace(',','')
        words_tokens=tokenizer.tokenize(var)
        words = [w for w in words_tokens if not w in stop_words] 
      
        Words=[]
        for w in words:
          if w.isdigit():
            Words.append(ps.stem(num2words(w)))
            
          else:
            Words.append(ps.stem(w))
            
      docs_list.append(tuple([Words,group]))

## Cell 13 ##
##Finding Accuracy##
actual=[]
predicted=[]
Global_unique=np.unique(GLOBAL)
B=len(Global_unique)
correct=0
for test in docs_list:
  max_arg=0
  p1=0
  p2=0
  p3=0
  p4=0
  p5=0
  for token in test[0]:
    if token in probab1.keys():
     # print(probab1.get(token))
      p1=p1+((probab1.get(token)+1)/(denominator[0]+B))
    if token in probab2.keys():
      p2=p2+((probab2.get(token)+1)/(denominator[1]+B))
    if token in probab3.keys():
      p3=p3+((probab3.get(token)+1)/(denominator[2]+B))
    if token in probab4.keys():
      p4=p4+((probab4.get(token)+1)/(denominator[3]+B))
    if token in probab5.keys():
      p5=p5+((probab5.get(token)+1)/(denominator[4]+B))

  T=list()
    
  T.append(p1)
  T.append(p2)
  T.append(p3)
  T.append(p4)
  T.append(p5)
  Index=T.index(max(T))
  actual.append(test[1])
  predicted.append(cat_list[Index])
  if cat_list[Index]==test[1]:
    correct=correct+1




      
accuracy=((correct*100)/len(docs_list))
print(accuracy)
pickle_out = open("drive/My Drive/apna.pickle","wb")
pickle.dump(correct, pickle_out)
pickle_out.close()

## Cell 14 ##
## Computing Confusion Matrix ##
from sklearn.metrics import confusion_matrix 

CM=confusion_matrix(actual, predicted)
print(CM)


##pickle_out = open("drive/My Drive/confusion_matrix_MI_Naive(70:30).pickle","wb")
#pickle.dump(CM, pickle_out)
#pickle_out.close()


#pickle_out = open("drive/My Drive/accuracy_MI_Naive(70:30).pickle","wb")
#pickle.dump(accuracy, pickle_out)
#pickle_out.close()