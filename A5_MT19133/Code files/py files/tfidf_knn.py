# -*- coding: utf-8 -*-
"""TFIDF_KNN_upd.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pxxz5O-4t_ATpelV-RHzloEiHubqjLfW
"""

## Cell 0 ##

cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
import pickle

#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# UNCOMMENT these three pickle files for executing the code for (80:20) split. Give path to the pickled files.  #

#pickle_in = open("PATH OF classes_list(80:20).pickle FILE","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF Global(80:20).pickle FILE","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("PATH OF classes_test_list(80:20).pickle FILE","rb")
#classes_test_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF tf_idf(80:20).pickle FILE","rb")
#tf_idf =pickle.load(pickle_in)



#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++






#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# UNCOMMENT these three pickle files for executing the code for 70:30 split. Give path to the pickled files. #

#pickle_in = open("PATH OF classes_list(70:30).pickle FILE","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF Global(70:30).pickle FILE","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("PATH OF classes_test_list(70:30).pickle FILE","rb")
#classes_test_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF tf_idf(70:30).pickle FILE","rb")
#tf_idf =pickle.load(pickle_in)



#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++





#++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

# UNCOMMENT these three pickle files for executing the code for (50:50) split. Give path to the pickled files. #

#pickle_in = open("PATH OF classes_list(50:50).pickle FILE","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF Global(50:50).pickle FILE","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("PATH OF classes_test_list(50:50).pickle FILE","rb")
#classes_test_list =pickle.load(pickle_in)

#pickle_in = open("PATH OF tf_idf(50:50).pickle FILE","rb")
#tf_idf =pickle.load(pickle_in)


#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++



#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
#Execution sequence of this python  code using pickled results

# Cell 1 --> Cell 2 --> Cell 3 -->Cell 4-->cell 11-->cell 12-->cell 14--->Cell 15--> Cell 16 --> Cell 18-->Cell 19--->Cell 20
#+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

### Cell 1 ###
##importing  nltk##

import nltk
nltk.download('punkt')

#Cell 2 #
##installing num2words ##
pip install num2words

#Cell 3 #

#Importing all esentials#
from nltk.stem import PorterStemmer 
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize 
import numpy as np
import glob
from collections import Counter 
  
import os
from num2words import num2words
from nltk.corpus import stopwords 
import pickle
import math
import operator
from sklearn.manifold import TSNE
import seaborn as sns
import matplotlib.pyplot as plt 
import random 
from sklearn.model_selection import train_test_split
import pandas as pd  
from sklearn.utils import shuffle
ps = PorterStemmer() 
tokenizer = RegexpTokenizer(r'\w+')

## CELL 4  ##
## Download stopwords ##

import nltk
nltk.download('stopwords')

## Cell 5 ##
##Splitting into Test & Train RANDOMLY using Test Train Split and shuffle ###

Data=pd.DataFrame(columns = ["file_name", "class"])
cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]


for group in cat_list:
  for filename in glob.glob("drive/My Drive/IR4/"+group+"/*"):
    
    length=len(filename)
    j=0
    for k in range(length-1,-1,-1):
      if filename[k]=='/':
        j=k
        break
  
    Data=Data.append({"file_name":filename[j+1:len(filename)],"class":group},ignore_index=True)
##Shuffling and split ##
Data = shuffle(Data)
x=Data["file_name"]
y=Data["class"]
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size = 0.5, random_state = 12)

## Cell 6 ##
#pickle_out = open("drive/My Drive/xTest(70:30).pickle","wb")
#pickle.dump(xTest, pickle_out)
#pickle_out.close()

##  Cell 7 ##
# # Train Data pre processing ##
stop_words = set(stopwords.words('english')) 
cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
GLOBAL=[]
classes_list={}
for group in cat_list:
  global_doc=[]
  for filename in glob.glob("drive/My Drive/IR4/"+group+"/*"):
    
    length=len(filename)
    j=0
    for k in range(length-1,-1,-1):
      if filename[k]=='/':
        j=k
        break
    print(filename[j+1:len(filename)])
    if filename[j+1:len(filename)] in list(xTrain):
      if not os.path.isdir(filename):
        text = open(filename,"r+",errors='ignore') 
        var=(text.read())
        var=var.replace(',','')
        words_tokens=tokenizer.tokenize(var)
        words = [w for w in words_tokens if not w in stop_words] 
      
        Words=[]
        for w in words:
          if w.isdigit():
            Words.append(ps.stem(num2words(w)))
            GLOBAL.append(ps.stem(num2words(w)))
          else:
            Words.append(ps.stem(w))
            GLOBAL.append(ps.stem(w))
      global_doc.append(Words)
  classes_list.update({group:global_doc})

## Cell 8 ##
#pickle_out = open("drive/My Drive/Global(70:30).pickle","wb")
#pickle.dump(GLOBAL, pickle_out)
#pickle_out.close()


#pickle_out = open("drive/My Drive/classes_list(70:30).pickle","wb")
#pickle.dump(classes_list, pickle_out)
#pickle_out.close()

## Cell 9 ##

### Feature Selection using TF-IDF ###

tf_idf=[]

Global_unique=np.unique(GLOBAL)

for term in Global_unique:
  df=0
  for key in classes_list.keys():
    docs=classes_list.get(key)
    for doc in docs:
      if term in doc:
        df=df+1
        break
  #print(df,term)
  df=math.log((5/df),10)  

  for clas in cat_list:
    tf=0
    docs=classes_list.get(clas)
    for doc in docs:
      tf=tf+doc.count(term)
    
    tf=1+math.log((tf+1),10)
    print(tf*df,term)
    tf_idf.append(tuple([term,clas,tf*df]))


##pickle_out = open("drive/My Drive/tf_idf(70:30).pickle","wb")
##pickle.dump(tf_idf, pickle_out)
##pickle_out.close()

## Cell 10 ##

## Fetch various essential variables which were pickled earliar ##


#cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
#pickle_in = open("drive/My Drive/classes_list(70:30).pickle","rb")
#classes_list =pickle.load(pickle_in)

#pickle_in = open("drive/My Drive/Global(70:30).pickle","rb")
#GLOBAL =pickle.load(pickle_in)

#pickle_in = open("drive/My Drive/classes_test_list(70:30).pickle","rb")
#classes_test_list =pickle.load(pickle_in)

#pickle_in = open("drive/My Drive/tf_idf(70:30).pickle","rb")
#tf_idf =pickle.load(pickle_in)

## Cell 11 ##
#### selecting the value of number of features.I have taken top 50% features from each class on the basis of highest tf idf score ###


cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]


Global_unique=np.unique(GLOBAL)

tf_idf1=[]
tf_idf2=[]
tf_idf3=[]
tf_idf4=[]
tf_idf5=[]

for item in tf_idf:
  if item[1]==cat_list[0]:
    #print("Hi1")
    tf_idf1.append(item)
  if item[1]==cat_list[1]:
    tf_idf2.append(item)
    #print("Hi2")
  if item[1]==cat_list[2]:
    tf_idf3.append(item)
   # print("Hi3")
  if item[1]==cat_list[3]:
    tf_idf4.append(item)
   # print("Hi4")
  if item[1]==cat_list[4]:
    tf_idf5.append(item)
   # print("Hi5")

tf_idf1.sort(key=lambda elem : elem[2],reverse=True)
tf_idf2.sort(key=lambda elem : elem[2],reverse=True)
tf_idf3.sort(key=lambda elem : elem[2],reverse=True)
tf_idf4.sort(key=lambda elem : elem[2],reverse=True)
tf_idf5.sort(key=lambda elem : elem[2],reverse=True)


length=(len(tf_idf1))
length=length/2

TF_IDF=[]
TF_IDF.append(tf_idf1[0:int(length)])
TF_IDF.append(tf_idf2[0:int(length)])
TF_IDF.append(tf_idf3[0:int(length)])
TF_IDF.append(tf_idf4[0:int(length)])
TF_IDF.append(tf_idf5[0:int(length)])


print(len(TF_IDF))
print(len(TF_IDF[0]))
print(len(TF_IDF[1]))
print(len(TF_IDF[2]))
print(len(TF_IDF[3]))

## Cell 12 ###
### Creating train vectors tuples(class,tf_idf vector) for each train document ###

train_vecs=[]
selected_features=[]

## making union of all selected features from each class ###
for cls in TF_IDF:
  for item in cls:
    selected_features.append(item[0])

selected_features=set(selected_features)
selected_features=list(selected_features)



for key in classes_list.keys():

  docs=classes_list.get(key)
  for doc in docs:
    #print(doc)
    vec=[]
    for feature in selected_features:
      vec.append(doc.count(feature))
    train_vecs.append(tuple([key,vec]))

## Cell 13 ##

####Pre -Processin test docs ###
#stop_words = set(stopwords.words('english')) 
cat_list=["comp.graphics","rec.sport.hockey","sci.med","sci.space","talk.politics.misc"]
#GLOBAL=[]
classes_test_list={}
for group in cat_list:
  global_doc=[]
  for filename in glob.glob("drive/My Drive/IR4/"+group+"/*"):
    
    length=len(filename)
    j=0
    for k in range(length-1,-1,-1):
      if filename[k]=='/':
        j=k
        break
    print(filename[j+1:len(filename)])
    if filename[j+1:len(filename)] in list(xTest):
      if not os.path.isdir(filename):
        text = open(filename,"r+",errors='ignore') 
        var=(text.read())
        var=var.replace(',','')
        words_tokens=tokenizer.tokenize(var)
        words = [w for w in words_tokens if not w in stop_words] 
      
        Words=[]
        for w in words:
          if w.isdigit():
            Words.append(ps.stem(num2words(w)))
            #GLOBAL.append(ps.stem(num2words(w)))
          else:
            Words.append(ps.stem(w))
            #GLOBAL.append(ps.stem(w))
      global_doc.append(Words)
  classes_test_list.update({group:global_doc})

#pickle_out = open("drive/My Drive/classes_test_list(70:30).pickle","wb")
#pickle.dump(classes_test_list, pickle_out)
#pickle_out.close()

## Cell 14 ##
### Creating Test vectors tuples(class,tf_idf vector) for each train document ###


test_vecs=[]

for key in classes_test_list.keys():

  docs=classes_test_list.get(key)
  for doc in docs:
    print(doc)
    vec=[]
    for feature in selected_features:
      vec.append(doc.count(feature))
    test_vecs.append(tuple([key,vec]))

## Cell 15 ##
#### Matrix multiplication of Test and Train docs for finding Cosine similarity ####
TestV=[]
TrainV=[]
for test in test_vecs:
  TestV.append(test[1])

for train in train_vecs:
  TrainV.append(train[1])


TestV=np.array(TestV)
TrainV=np.array(TrainV)
dot=np.dot(TestV,TrainV.T)

## Cell 16 ##
## Finding Similarity Matrix for each Test vector corresponding to every Train vector ##
similarity_matrix=[]
for i in range(0,len(dot)):
  mag_test=np.linalg.norm(TestV[i])
  print(i)
  list1=dot[i]
  list1=list1/mag_test
  #print(len(list))
  temp=[]
  for j in range(0,len(train_vecs)):
    
    mag_train=np.linalg.norm(TrainV[j])
    temp.append(tuple([j,list1[j]/mag_train]))

  similarity_matrix.append(temp)

## Cell 17 ##
#similarity_matrix=dot
##pickle_out = open("drive/My Drive/similarity_matrix_TFIDF_KNN(70:30).pickle","wb")
##pickle.dump(similarity_matrix, pickle_out)
##pickle_out.close()

#pickle_out = open("drive/My Drive/reecha.pickle","wb")
#pickle.dump(similarity_matrix, pickle_out)
#pickle_out.close()

## Cell 18 ##
## KNN logic##
import statistics 
  
i=0
ct=0
actual=[]
predicted=[]
for test in similarity_matrix:
  
  
  item=sorted(test, key=lambda tup: tup[1],reverse=True)
  

  ## In order to select the k value of KNN change the second value of the below line of code. Say for K=5 below line will be--> item=item[0:5] 
  ## For K=3 below line will be--> item=item[0:3]  ###
  
  item=item[0:5]
  
  
  vecs=[]
  for it in item:
    x=it[0]
    vecs.append(train_vecs[x][0])

  res=Counter(vecs)
  
  class1=res.most_common(1)[0][0]
  predicted.append(class1)
  actual.append(test_vecs[i][0])
  if class1==test_vecs[i][0]:
    ct=ct+1
  i=i+1

acrcy=((ct*100)/len(similarity_matrix))
print(acrcy)

##Cell 19  ###
## Creating Confusion Matrix ###

from sklearn.metrics import confusion_matrix 

CM=confusion_matrix(actual, predicted)
print(CM)


#pickle_out = open("drive/My Drive/confusion_matrix_TFIDF_KNN(k=5)(70:30).pickle","wb")
#pickle.dump(CM, pickle_out)
#pickle_out.close()


#pickle_out = open("drive/My Drive/accuracy_TFIDF_KNN(k=5)(70:30).pickle","wb")
#pickle.dump(acrcy, pickle_out)
#pickle_out.close()

## Cell 20##

## give value of accuracy in list y [] to plot graph for "Number of neighbours vs accuracy in TF-IDF & KNN"  ###

import matplotlib.pyplot as plt 


x=[1,3,5]
y=[92.28,93.12,92.84]  
tick_label = ['K=1', 'K=3', 'K=5'] 

plt.bar(x,y, tick_label = tick_label, 
		width = 0.8, color = ['red', 'green','blue']) 


plt.xlabel('number of neighbours') 

plt.ylabel('accuracy') 

plt.title('Number of neighbours vs accuracy in TF-IDF & KNN') 
plt.ylim(90,95) 
#plt.xlim(1,8) 

plt.show()