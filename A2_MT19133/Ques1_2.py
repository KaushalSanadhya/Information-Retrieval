# -*- coding: utf-8 -*-
"""IR_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DMGwrUbDeGNvVQgJS8pXZw8ldHOOlJ4W
"""

import nltk
nltk.download('punkt')

from nltk.stem import PorterStemmer 
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize 
ps = PorterStemmer() 

tokenizer = RegexpTokenizer(r'\w+')

text = open("sample_data/stopwords.txt","r+",errors='ignore') 
var=(text.read())
words=tokenizer.tokenize(var)
stop_words=[]

for word in words:
  stop_words.append(ps.stem(word))

print(stop_words)

pip install num2words

import glob
import csv
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer 
from num2words import num2words
from nltk.tokenize import word_tokenize 
ps = PorterStemmer() 
tokenizer = RegexpTokenizer(r'\w+')
q_stem=[]
query=input("please input your query")
query=query.replace(',','')
    
q_words=tokenizer.tokenize(query)
for q_w in q_words:
  if q_w.isdigit():
  
    q_stem.append(ps.stem(num2words(q_w)))
  else:
    q_stem.append(ps.stem(q_w))
q_stem = list(dict.fromkeys(q_stem))
q_stem=set(q_stem)
import math  
import os
Dict = {} 
tokenizer = RegexpTokenizer(r'\w+')
for filename in glob.glob("*"):
  if not os.path.isdir(filename):
    text = open(filename,"r+",errors='ignore') 
    var=(text.read())
    var=var.replace(',','')
    

    words=tokenizer.tokenize(var)
    doc_stem=[]
    count=0
    for w in words:
      if w.isdigit():
        #w=num2words(w)
        doc_stem.append(ps.stem(num2words(w)))
      else:
        doc_stem.append(ps.stem(w))  
    doc_stem = list(dict.fromkeys(doc_stem))
    doc_stem=set(doc_stem)
    #for word in doc_stem:
    #  if word in q_stem:
    #    count=count+1
    #print(filename)
    #print(len(q_stem.union(doc_stem)))
    #print((q_stem.intersection(doc_stem)))
    j_c=len(q_stem.intersection(doc_stem))/math.sqrt(len(q_stem.union(doc_stem)))
    #j_c=count/(len(doc_stem)+len(q_stem)-count)
    #j_c=len(q_stem.intersection(doc_stem))
    Dict.update({filename:j_c})


      


#l=list(Dict.values())
#l.sort(reverse=True) 

#Dict=dict(l)  

print("Dictionary",Dict)

Dict=sorted(Dict.items(), key = lambda x : x[1],reverse=True)
print(Dict)

K=input("Enter number of documents to be returned")
K=int(K)
for i in range(0,K):
  print(Dict[i])

import glob
import csv
from nltk.tokenize import RegexpTokenizer
from nltk.stem import PorterStemmer 
from nltk.tokenize import word_tokenize 
from num2words import num2words

ps = PorterStemmer() 
  
import os
count=0
count1=0
Dict_tokens = {} 
Dict_docs={}
Dict_df={}
Dict_flag={}
tokenizer = RegexpTokenizer(r'\w+')
for filename in glob.glob("*"):
  if not os.path.isdir(filename):

    for term in Dict_flag:
      Dict_flag.pop(term)
      Dict_flag.update({term:0})
    
    if filename not in Dict_docs:
      Dict_docs.update({filename:count1})
      count1=count1+1
    text = open(filename,"r+",errors='ignore') 
    var=(text.read())
    var=var.replace(',','')

    words=tokenizer.tokenize(var)
    Temp=[]
    for w in words:
      if w.isdigit():
        w=num2words(w)      
      Temp.append(ps.stem(w))  
    for word in Temp:
      if word not in Dict_tokens:
        Dict_tokens.update({word:count})
        count=count+1
      if word not in Dict_df:
        Dict_df.update({word:1})
        #print(word)
        Dict_flag.update({word:1})
      else :
        if Dict_flag.get(word) == 0:
          c=Dict_df.get(word)
          c=c+1
          Dict_df.pop(word)
          Dict_df.update({word:c})
          Dict_flag.update({word:1})

print(Dict_tokens)

print(Dict_df)

print(len(Dict_docs))

for doc in Dict_docs:
  print(doc)

###First Variation####
import math
from num2words import num2words
query=input("Enter your query")
query=query.replace(',','')
k=input("How many top results you wish to find")
k=int(k)
query_stem_original=[]

words=tokenizer.tokenize(query)

for w in words:
  if w.isdigit():
  
    query_stem_original.append(ps.stem(num2words(w))) 
  else:
    query_stem_original.append(ps.stem(w))  
  
query_stem = list(dict.fromkeys(query_stem_original))
global_array={}

for doc in Dict_docs:
  Temp=[]
  text = open(doc,"r+",errors='ignore') 
  var=(text.read())
  var=var.replace(',','')

  words=tokenizer.tokenize(var)
  
  for w in words:
    if w.isdigit():
      Temp.append(ps.stem(num2words(w))) 
    else:
      Temp.append(ps.stem(w))  
  tf_idf=0
  for q_t in query_stem:
    count=0
    if q_t in Temp:
      
      for term in Temp:
        if q_t ==term:
          count=count+1
    tf=count
    tf_idf=tf_idf + ( tf*math.log(len(Dict_docs)/int(Dict_df.get(q_t))) )
  #print(tf_idf)  
  global_array.update({doc:tf_idf})

##################################
print(query_stem)
#print(global_array)                                         
import operator
Dict= dict( sorted(global_array.items(), key=operator.itemgetter(1),reverse=True))
print(Dict) 
for item in Dict:
  print(item)
  k=k-1
  if k<=0:
    break

print(Dict.get('alissadl.txt'))

###Variation 2 
##tf (logarithm) 1 + log(tft,d)
###idf =1
from num2words import num2words
import math
query=input("Enter your query")
query=query.replace(',','')
k=input("How many top results you wish to find")
k=int(k)
query_stem_original=[]

words=tokenizer.tokenize(query)

for w in words:
  if w.isdigit():
  
    query_stem_original.append(ps.stem(num2words(w))) 
  else:
    query_stem_original.append(ps.stem(w))  
  
query_stem = list(dict.fromkeys(query_stem_original))
global_array={}

for doc in Dict_docs:
  Temp=[]
  text = open(doc,"r+",errors='ignore') 
  var=(text.read())
  var=var.replace(',','')

  words=tokenizer.tokenize(var)
  
  for w in words:
    if w.isdigit():
      Temp.append(ps.stem(num2words(w))) 
    else:
      Temp.append(ps.stem(w))  
    
  tf_idf=0
  for q_t in query_stem:
    count=0
    if q_t in Temp:
      
      for term in Temp:
        if q_t ==term:
          count=count+1
    tf=1+math.log(1+count)
    tf_idf=tf_idf + ( tf*math.log(len(Dict_docs)/int(Dict_df.get(q_t)) ))

  global_array.update({doc:tf_idf})


##################################

                                         
import operator
Dict= dict( sorted(global_array.items(), key=operator.itemgetter(1),reverse=True))
print(Dict) 
for item in Dict:
  print(item)
  k=k-1
  if k<=0:
    break

###Variation 2 
##tf (logarithm) 1 + log(tft,d)
###idf =1
from num2words import num2words
import math
query=input("Enter your query")
query=query.replace(',','')
k=input("How many top results you wish to find")
k=int(k)
query_stem_original=[]

words=tokenizer.tokenize(query)

for w in words:
  if w.isdigit():
  
    query_stem_original.append(ps.stem(num2words(w))) 
  else:
    query_stem_original.append(ps.stem(w))  
  
query_stem = list(dict.fromkeys(query_stem_original))
global_array={}

for doc in Dict_docs:
  Temp=[]
  text = open(doc,"r+",errors='ignore') 
  var=(text.read())
  var=var.replace(',','')

  words=tokenizer.tokenize(var)
  
  for w in words:
    if w.isdigit():
      Temp.append(ps.stem(num2words(w))) 
    else:
      Temp.append(ps.stem(w))  
    
  tf_idf=0
  for q_t in query_stem:
    count=0
    if q_t in Temp:
      
      for term in Temp:
        if q_t ==term:
          count=count+1
    if count >0 :
      tf=1
    else :
      tf=0
    tf_idf=tf_idf + ( tf*math.log(len(Dict_docs)/int(Dict_df.get(q_t)) ))

  global_array.update({doc:tf_idf})


##################################

                                         
import operator
Dict= dict( sorted(global_array.items(), key=operator.itemgetter(1),reverse=True))
print(Dict) 
for item in Dict:
  print(item)
  k=k-1
  if k<=0:
    break

###Variation 3 
##tf (boolean) 
###idf =(logrithmic)
from num2words import num2words
import math
query=input("Enter your query")
query=query.replace(',','')
k=input("How many top results you wish to find")
k=int(k)
query_stem_original=[]

words=tokenizer.tokenize(query)

for w in words:
  if w.isdigit():
  
    query_stem_original.append(ps.stem(num2words(w))) 
  else:
    query_stem_original.append(ps.stem(w))  
  
query_stem = list(dict.fromkeys(query_stem_original))
global_array={}

for doc in Dict_docs:
  Temp=[]
  text = open(doc,"r+",errors='ignore') 
  var=(text.read())
  var=var.replace(',','')

  words=tokenizer.tokenize(var)
  
  for w in words:
    if w.isdigit():
      Temp.append(ps.stem(num2words(w))) 
    else:
      Temp.append(ps.stem(w))  
    
  tf_idf=0
  for q_t in query_stem:
    count=0
    if q_t in Temp:
      
      for term in Temp:
        if q_t ==term:
          count=count+1
    if count > 0:
      tf=1
    else:
      tf=0
    
    tf_idf=tf_idf +(1* ( tf*math.log(len(Dict_docs)/int(Dict_df.get(q_t)) )))

    


 
  global_array.update({doc:tf_idf})


##################################

                                         
import operator
Dict= dict( sorted(global_array.items(), key=operator.itemgetter(1),reverse=True))
print(Dict) 
for item in Dict:
  print(item)
  k=k-1
  if k<=0:
    break

query=input("Enter your query")
query=query.replace(',','')

query_stem_original=[]

words=tokenizer.tokenize(query)

for w in words:
  if w.isdigit():
    query_stem_original.append(ps.stem(num2words(w)))
  else:  
    query_stem_original.append(ps.stem(w))  
  
query_stem = list(dict.fromkeys(query_stem_original))
Q_A=[]


print(query_stem)
for t in query_stem:

  count=0
  tf_idf=0
  for term in query_stem_original:
    if t==term:
      count=count+1
  #tf=1+math.log(1+count)
  tf=count
  tf_idf=tf*math.log(len(Dict_docs)/int(Dict_df.get(t)))
  Q_A.append(tf_idf)
  

print(Q_A)

import re
Dict_map={}
file = open("sample_data/index.html", 'r')
text = file.read().strip()
file.close()
file_name = re.findall('><A HREF="(.*)">', text)
file_title = re.findall('<BR><TD> (.*)\n', text)
file_name.pop(0)
file_name.pop(0)

i=0
for item in file_name:
  Dict_map.update({item:file_title[i]})
  i=i+1
print(Dict_map)

print(file_name)

import numpy as np
Q_A=np.array(Q_A)
print(Q_A)
mod_q=0
for item in Q_A:
  mod_q=mod_q+ item**2

mod_q=math.sqrt(mod_q)
print(mod_q)

######Cosine Similarity######
import numpy as np
from num2words import num2words
Dict_sim={}
Q_A_new=[]
for item in Q_A:
  Q_A_new.append(item/mod_q)

for doc in Dict_docs:
  Temp=[]
  D_A=[]
  text = open(doc,"r+",errors='ignore') 
  var=(text.read())
  var=var.replace(',','')

  words=tokenizer.tokenize(var)
  
  for w in words:
    if w.isdigit():
      Temp.append(ps.stem(num2words(w)))
    else:

      Temp.append(ps.stem(w))  
  tf_idf=0
  for q_t in query_stem:
    count=0
    if q_t in Temp:
      
      for term in Temp:
        if q_t ==term:
          count=count+1
    #tf=1+math.log(1+count)
    tf=count
    tf_idf=( tf*math.log(len(Dict_docs)/int(Dict_df.get(q_t))) )
    D_A.append(tf_idf)
  D_A=np.array(D_A)
  mod_d=0
  D_A_new=[]
  for item in D_A:
    mod_d=mod_d+ item**2

  mod_d=math.sqrt(mod_d)

  for item in D_A:
    D_A_new.append(item/mod_d)

  similarity=0
  for i in range(0,len(D_A)):
    similarity=similarity+ (D_A[i]*Q_A[i])

  similarity=similarity
  Dict_sim.update({doc:similarity})
  #print(doc)
  #print(D_A)
  #print(mod_d)
  #print(mod_q)


import operator
Dict_sim= dict( sorted(Dict_sim.items(), key=operator.itemgetter(1),reverse=True)  ) 

print(Dict_sim)

k=20
for item in Dict_sim:
  print(item)
  k=k-1
  if k<=0:
    break

text = open("sample_data/english2.txt","r+",errors='ignore') 
var=(text.read())
words=tokenizer.tokenize(var)

##########################################################
###################### Question 2 ######################## 
##########################################################


def func(A,k):
  l=[]
  for term in words:

    B=term

    m=len(A)
    n=len(B)
    Edit=[[0 for i in range(0,n+1)]for j in range(0,m+1)]

    for j in range(0,n+1):
      Edit[0][j]=j

    for i in range(1,m+1):
      Edit[i][0]=i
      for j in range(1,n+1):
        mylist=[]
        ins=Edit[i][j-1]+2
        
        de=Edit[i-1][j]+1
        if A[i-1]==B[j-1]:
          rep=Edit[i-1][j-1]
        else:
          rep=Edit[i-1][j-1]+3
        mylist.append(ins)
        mylist.append(de)
        mylist.append(rep)
        minimum=min(mylist)
        
        Edit[i][j]=minimum
        
    if Edit[m][n]<=k:
      l.append(B)
  return l

query=input("Enter your query")
q_w=tokenizer.tokenize(query)
query_words=[]
for t in q_w:
  query_words.append(t.lower())
  

k=input("Enter k")
k=int(k)
for term in query_words:
  if term not in words:
    l=func(term,k)
    print("suggestion for",term,"are:")
    print(l)

